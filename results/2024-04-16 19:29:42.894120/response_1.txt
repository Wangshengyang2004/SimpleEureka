```python
def calculate_metrics(self) -> None:
    # Calculate distance to target
    target_dist = torch.norm(self.root_pos - self.target_positions, dim=1)
    
    # Reward for being close to the target
    pos_reward = torch.exp(-3 * target_dist)
    
    # Reward for having the correct orientation
    forward_vector = torch.tensor([0, 0, 1.0], device=self.device)
    orientation_reward = ((self.root_quats @ forward_vector.unsqueeze(0))[:, 2] + 1) / 2
    
    # Reward for fast flipping
    speed = torch.norm(self.root_velocities[:, :3], dim=1)
    flip_reward = torch.exp(-5 * (speed - 2.0) ** 2)
    
    # Combine all rewards
    total_reward = pos_reward * orientation_reward * flip_reward
    
    # Store the individual rewards
    reward_dict = {
        "pos_reward": pos_reward,
        "orientation_reward": orientation_reward,
        "flip_reward": flip_reward,
        "total_reward": total_reward
    }
    
    return total_reward, reward_dict

def is_done(self) -> None:
    # Check if the drone has reached the target or gone too far
    done = (self.target_dist < 0.5) | (self.target_dist > 2.5)
    
    # Check if the drone has flipped correctly
    done |= (self.flip_reward < 0.5)
    
    # Check if the episode length has reached its maximum
    done |= (self.progress_buf >= self._max_episode_length - 1)
    
    return done

# Additional variables for the observation space
new_episode_sums = {
    "rew_pos": torch.zeros(self._num_envs, device=self.device),
    "rew_orient": torch.zeros(self._num_envs, device=self.device),
    "rew_flip": torch.zeros(self._num_envs, device=self.device),
    "raw_dist": torch.zeros(self._num_envs, device=self.device),
    "raw_orient": torch.zeros(self._num_envs, device=self.device),
    "raw_flip": torch.zeros(self._num_envs, device=self.device)
}
```
#END