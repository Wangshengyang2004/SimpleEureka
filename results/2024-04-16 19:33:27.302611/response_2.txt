```python
    # velocity for spinning
    desired_y_angvel = 2 * math.pi / 3.5  # Desired spin rate around Y-axis
    y_angvel_diff = y_angvel - desired_y_angvel
    y_penalty = torch.square(y_angvel_diff).sum(-1)

    # Combine penalties with weights
    combined_penalty = 0.1 * torch.exp(-1.0 * y_penalty) + 0.05 * torch.exp(-1.0 * roll_penalty) + 0.05 * torch.exp(-1.0 * yaw_penalty)

    # Update spin_reward with the combined penalty
    spin_reward = combined_penalty

    # Combined reward
    self.rew_buf[:] = pos_reward * up_reward * speed_reward * coline_reward * spin_reward

    # Log episode reward sums
    self.episode_sums["rew_pos"] += pos_reward
    self.episode_sums["rew_orient"] += up_reward
    self.episode_sums["rew_speed"] += speed_reward
    self.episode_sums["rew_coline"] += coline_reward
    self.episode_sums["rew_spin"] += spin_reward

    # Log raw information
    self.episode_sums["raw_dist"] += target_dist
    self.episode_sums["raw_orient"] += ups[..., 2]

def is_done(self) -> None:
    # Resets due to misbehavior
    ones = torch.ones_like(self.reset_buf)
    die = torch.zeros_like(self.reset_buf)
    die = torch.where(self.target_dist > 10.0, ones, die)

    # z >= 0.5 & z <= 5.0 & up > 0
    die = torch.where(self.root_positions[..., 2] < -2.0, ones, die)
    die = torch.where(self.root_positions[..., 2] > 6.0, ones, die)

    # Resets due to episode length
    self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die)

#END
```