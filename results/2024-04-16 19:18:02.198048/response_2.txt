```python
        2]  # Extract yaw component

        # Desired angular velocities for roll and yaw are zero (minimal movement)
        desired_roll_angvel = 0.0
        desired_yaw_angvel = 0.0

        # Calculate the penalties for deviation from zero for roll and yaw
        roll_penalty = torch.square(roll_angvel - desired_roll_angvel).sum(-1)
        yaw_penalty = torch.square(yaw_angvel - desired_yaw_angvel).sum(-1)

        y_angvel = root_angvels[:, 1]  # Y-axis angular velocity for spinning
        desired_y_angvel = 2 * math.pi / 3.5  # Desired spin rate around Y-axis
        y_angvel_diff = y_angvel - desired_y_angvel
        y_penalty = torch.square(y_angvel_diff).sum(-1)

        # Higher weight for Y-axis spin reward, lower weight for roll and yaw penalties to encourage minimal movements
        combined_penalty = 0.1 * torch.exp(-1.0 * y_penalty) + 0.05 * torch.exp(-1.0 * roll_penalty) + 0.05 * torch.exp(-1.0 * yaw_penalty)

        # Update spin_reward with the combined penalty
        spin_reward = combined_penalty

        # combined reward
        self.rew_buf[:] = (pos_reward + speed_reward + coline_reward) * (1 + up_reward + spin_reward)

        # log episode reward sums
        self.episode_sums["rew_pos"] += pos_reward
        self.episode_sums["rew_orient"] += up_reward
        self.episode_sums["rew_speed"] += speed_reward
        self.episode_sums["rew_coline"] += coline_reward
        self.episode_sums["rew_spin"] += spin_reward

        # log raw info
        self.episode_sums["raw_dist"] += target_dist
        self.episode_sums["raw_orient"] += ups[..., 2]

    def is_done(self) -> None:
        # resets due to misbehavior
        ones = torch.ones_like(self.reset_buf)
        die = torch.zeros_like(self.reset_buf)
        die = torch.where(self.target_dist > 10.0, ones, die)

        # z >= 0.5 & z <= 5.0 & up > 0
        die = torch.where(self.root_positions[..., 2] < -2.0, ones, die)
        die = torch.where(self.root_positions[..., 2] > 6.0, ones, die)

        # resets due to episode length
        self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die)  #END
```
This code provides the required functions for the reinforcement learning task, including the `target_position_rot` function for calculating target positions, the `calculate_metrics` function for computing individual reward components and the total reward, and the `is_done` function for determining when an episode should end. The code is designed to be compatible with the existing environment class definition and uses PyTorch tensors for all operations.