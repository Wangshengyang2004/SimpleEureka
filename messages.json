[
    {
        "role": "system",
        "content": "##### System Prompt\n\nYou are a reinforcement learning expert and a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible. Your fellow is the product manager. \n\nYour ultimate goal is to write a reward function for the environment that will help the agent learn the task described in text. \n\nYour reward function should use useful variables from the environment as inputs. As an example,\n\nthe reward function signature can be: \n\n  def calculate_metrics(self): -> None:\n\nThe final lines of the reward function should consist of two items:\n\n  (1) compute the total reward,\n\n  (2) a dictionary of each individual reward component and raw information that are added into lists.\n\nThe code output should be formatted as a python code string: \"`python ... `\".\n\nPlus, your output should strictly include these functions:\n\n1. Function that you newly created for calculation\n2. compute_metrics function\n\nYou shouldn't include other parts of code because the tokens are expansive."
    },
    {
        "role": "user",
        "content": "The Python environment is import numpy as np\nimport math\nimport torch\nfrom omni.isaac.core.objects import DynamicSphere\nfrom omni.isaac.core.prims import RigidPrimView\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.torch.rotations import *\nfrom omniisaacgymenvs.tasks.base.rl_task import RLTask\nfrom omniisaacgymenvs.robots.articulations.crazyflie import Crazyflie\nfrom omniisaacgymenvs.robots.articulations.views.crazyflie_view import CrazyflieView\n\nEPS = 1e-6  # small constant to avoid divisions by 0 and log(0)\n\n\nclass CrazyflieTask(RLTask):\n    def __init__(self, name, sim_config, env, offset=None) -> None:\n        self.update_config(sim_config)\n\n        self._num_observations = 18\n        self._num_actions = 4\n\n        self._crazyflie_position = torch.tensor([0, 0, 1.0])\n        self._ball_position = torch.tensor([0, 0, 1.0])\n\n        RLTask.__init__(self, name=name, env=env)\n\n        return\n\n    def update_config(self, sim_config):\n        self._sim_config = sim_config\n        self._cfg = sim_config.config\n        self._task_cfg = sim_config.task_config\n\n        self._num_envs = self._task_cfg[\"env\"][\"numEnvs\"]\n        self._env_spacing = self._task_cfg[\"env\"][\"envSpacing\"]\n        self._max_episode_length = self._task_cfg[\"env\"][\"maxEpisodeLength\"]\n\n        self.dt = self._task_cfg[\"sim\"][\"dt\"]\n\n        # parameters for the crazyflie\n        self.arm_length = 0.05\n\n        # parameters for the controller\n        self.motor_damp_time_up = 0.15\n        self.motor_damp_time_down = 0.15\n\n        # I use the multiplier 4, since 4*T ~ time for a step response to finish, where\n        # T is a time constant of the first-order filter\n        self.motor_tau_up = 4 * self.dt / (self.motor_damp_time_up + EPS)\n        self.motor_tau_down = 4 * self.dt / (self.motor_damp_time_down + EPS)\n\n        # thrust max\n        self.mass = 0.028\n        self.thrust_to_weight = 1.9\n\n        self.motor_assymetry = np.array([1.0, 1.0, 1.0, 1.0])\n        # re-normalizing to sum-up to 4\n        self.motor_assymetry = self.motor_assymetry * 4.0 / np.sum(self.motor_assymetry)\n\n        self.grav_z = -1.0 * self._task_cfg[\"sim\"][\"gravity\"][2]\n\n    def set_up_scene(self, scene) -> None:\n        self.get_crazyflie()\n        self.get_target()\n        RLTask.set_up_scene(self, scene)\n        self._copters = CrazyflieView(prim_paths_expr=\"/World/envs/.*/Crazyflie\", name=\"crazyflie_view\")\n        self._balls = RigidPrimView(prim_paths_expr=\"/World/envs/.*/ball\", name=\"ball_view\")\n        scene.add(self._copters)\n        scene.add(self._balls)\n        for i in range(4):\n            scene.add(self._copters.physics_rotors[i])\n        return\n\n    def initialize_views(self, scene):\n        super().initialize_views(scene)\n        if scene.object_exists(\"crazyflie_view\"):\n            scene.remove_object(\"crazyflie_view\", registry_only=True)\n        if scene.object_exists(\"ball_view\"):\n            scene.remove_object(\"ball_view\", registry_only=True)\n        for i in range(1, 5):\n            scene.remove_object(f\"m{i}_prop_view\", registry_only=True)\n        self._copters = CrazyflieView(prim_paths_expr=\"/World/envs/.*/Crazyflie\", name=\"crazyflie_view\")\n        self._balls = RigidPrimView(prim_paths_expr=\"/World/envs/.*/ball\", name=\"ball_view\")\n        scene.add(self._copters)\n        scene.add(self._balls)\n        for i in range(4):\n            scene.add(self._copters.physics_rotors[i])\n\n    def get_crazyflie(self):\n        copter = Crazyflie(\n            prim_path=self.default_zero_env_path + \"/Crazyflie\", name=\"crazyflie\", translation=self._crazyflie_position\n        )\n        self._sim_config.apply_articulation_settings(\n            \"crazyflie\", get_prim_at_path(copter.prim_path), self._sim_config.parse_actor_config(\"crazyflie\")\n        )\n\n    def get_target(self):\n        radius = 0.2\n        color = torch.tensor([1, 0, 0])\n        ball = DynamicSphere(\n            prim_path=self.default_zero_env_path + \"/ball\",\n            translation=self._ball_position,\n            name=\"target_0\",\n            radius=radius,\n            color=color,\n        )\n        self._sim_config.apply_articulation_settings(\n            \"ball\", get_prim_at_path(ball.prim_path), self._sim_config.parse_actor_config(\"ball\")\n        )\n        ball.set_collision_enabled(False)\n\n    def get_observations(self) -> dict:\n        self.root_pos, self.root_rot = self._copters.get_world_poses(clone=False)\n        self.root_velocities = self._copters.get_velocities(clone=False)\n\n        root_positions = self.root_pos - self._env_pos\n        root_quats = self.root_rot\n\n        rot_x = quat_axis(root_quats, 0)\n        rot_y = quat_axis(root_quats, 1)\n        rot_z = quat_axis(root_quats, 2)\n\n        root_linvels = self.root_velocities[:, :3]\n        root_angvels = self.root_velocities[:, 3:]\n\n        self.obs_buf[..., 0:3] = self.target_positions - root_positions\n\n        self.obs_buf[..., 3:6] = rot_x\n        self.obs_buf[..., 6:9] = rot_y\n        self.obs_buf[..., 9:12] = rot_z\n\n        self.obs_buf[..., 12:15] = root_linvels\n        self.obs_buf[..., 15:18] = root_angvels\n\n        observations = {self._copters.name: {\"obs_buf\": self.obs_buf}}\n        return observations\n\n    def pre_physics_step(self, actions) -> None:\n        if not self.world.is_playing():\n            return\n\n        reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)\n        if len(reset_env_ids) > 0:\n            self.reset_idx(reset_env_ids)\n\n        set_target_ids = (self.progress_buf % 500 == 0).nonzero(as_tuple=False).squeeze(-1)\n        if len(set_target_ids) > 0:\n            self.set_targets(set_target_ids)\n\n        actions = actions.clone().to(self._device)\n        self.actions = actions\n\n        # clamp to [-1.0, 1.0]\n        thrust_cmds = torch.clamp(actions, min=-1.0, max=1.0)\n        # scale to [0.0, 1.0]\n        thrust_cmds = (thrust_cmds + 1.0) / 2.0\n        # filtering the thruster and adding noise\n        motor_tau = self.motor_tau_up * torch.ones((self._num_envs, 4), dtype=torch.float32, device=self._device)\n        motor_tau[thrust_cmds < self.thrust_cmds_damp] = self.motor_tau_down\n        motor_tau[motor_tau > 1.0] = 1.0\n\n        # Since NN commands thrusts we need to convert to rot vel and back\n        thrust_rot = thrust_cmds**0.5\n        self.thrust_rot_damp = motor_tau * (thrust_rot - self.thrust_rot_damp) + self.thrust_rot_damp\n        self.thrust_cmds_damp = self.thrust_rot_damp**2\n\n        ## Adding noise\n        thrust_noise = 0.01 * torch.randn(4, dtype=torch.float32, device=self._device)\n        thrust_noise = thrust_cmds * thrust_noise\n        self.thrust_cmds_damp = torch.clamp(self.thrust_cmds_damp + thrust_noise, min=0.0, max=1.0)\n\n        thrusts = self.thrust_max * self.thrust_cmds_damp\n\n        # thrusts given rotation\n        root_quats = self.root_rot\n        rot_x = quat_axis(root_quats, 0)\n        rot_y = quat_axis(root_quats, 1)\n        rot_z = quat_axis(root_quats, 2)\n        rot_matrix = torch.cat((rot_x, rot_y, rot_z), 1).reshape(-1, 3, 3)\n\n        force_x = torch.zeros(self._num_envs, 4, dtype=torch.float32, device=self._device)\n        force_y = torch.zeros(self._num_envs, 4, dtype=torch.float32, device=self._device)\n        force_xy = torch.cat((force_x, force_y), 1).reshape(-1, 4, 2)\n        thrusts = thrusts.reshape(-1, 4, 1)\n        thrusts = torch.cat((force_xy, thrusts), 2)\n\n        thrusts_0 = thrusts[:, 0]\n        thrusts_0 = thrusts_0[:, :, None]\n\n        thrusts_1 = thrusts[:, 1]\n        thrusts_1 = thrusts_1[:, :, None]\n\n        thrusts_2 = thrusts[:, 2]\n        thrusts_2 = thrusts_2[:, :, None]\n\n        thrusts_3 = thrusts[:, 3]\n        thrusts_3 = thrusts_3[:, :, None]\n\n        mod_thrusts_0 = torch.matmul(rot_matrix, thrusts_0)\n        mod_thrusts_1 = torch.matmul(rot_matrix, thrusts_1)\n        mod_thrusts_2 = torch.matmul(rot_matrix, thrusts_2)\n        mod_thrusts_3 = torch.matmul(rot_matrix, thrusts_3)\n\n        self.thrusts[:, 0] = torch.squeeze(mod_thrusts_0)\n        self.thrusts[:, 1] = torch.squeeze(mod_thrusts_1)\n        self.thrusts[:, 2] = torch.squeeze(mod_thrusts_2)\n        self.thrusts[:, 3] = torch.squeeze(mod_thrusts_3)\n\n        # clear actions for reset envs\n        self.thrusts[reset_env_ids] = 0\n\n        # spin spinning rotors\n        prop_rot = self.thrust_cmds_damp * self.prop_max_rot\n        self.dof_vel[:, 0] = prop_rot[:, 0]\n        self.dof_vel[:, 1] = -1.0 * prop_rot[:, 1]\n        self.dof_vel[:, 2] = prop_rot[:, 2]\n        self.dof_vel[:, 3] = -1.0 * prop_rot[:, 3]\n\n        self._copters.set_joint_velocities(self.dof_vel)\n\n        # apply actions\n        for i in range(4):\n            self._copters.physics_rotors[i].apply_forces(self.thrusts[:, i], indices=self.all_indices)\n\n    def post_reset(self):\n        thrust_max = self.grav_z * self.mass * self.thrust_to_weight * self.motor_assymetry / 4.0\n        self.thrusts = torch.zeros((self._num_envs, 4, 3), dtype=torch.float32, device=self._device)\n        self.thrust_cmds_damp = torch.zeros((self._num_envs, 4), dtype=torch.float32, device=self._device)\n        self.thrust_rot_damp = torch.zeros((self._num_envs, 4), dtype=torch.float32, device=self._device)\n        self.thrust_max = torch.tensor(thrust_max, device=self._device, dtype=torch.float32)\n\n        self.motor_linearity = 1.0\n        self.prop_max_rot = 433.3\n\n        self.target_positions = torch.zeros((self._num_envs, 3), device=self._device, dtype=torch.float32)\n        self.target_positions[:, 2] = 1\n        self.actions = torch.zeros((self._num_envs, 4), device=self._device, dtype=torch.float32)\n\n        self.all_indices = torch.arange(self._num_envs, dtype=torch.int32, device=self._device)\n\n        # Extra info\n        self.extras = {}\n\n        torch_zeros = lambda: torch.zeros(self.num_envs, dtype=torch.float, device=self.device, requires_grad=False)\n        self.episode_sums = {\n            \"rew_pos\": torch_zeros(),\n            \"rew_orient\": torch_zeros(),\n            \"rew_effort\": torch_zeros(),\n            \"rew_spin\": torch_zeros(),\n            \"raw_dist\": torch_zeros(),\n            \"raw_orient\": torch_zeros(),\n            \"raw_effort\": torch_zeros(),\n            \"raw_spin\": torch_zeros(),\n        }\n\n        self.root_pos, self.root_rot = self._copters.get_world_poses()\n        self.root_velocities = self._copters.get_velocities()\n        self.dof_pos = self._copters.get_joint_positions()\n        self.dof_vel = self._copters.get_joint_velocities()\n\n        self.initial_ball_pos, self.initial_ball_rot = self._balls.get_world_poses(clone=False)\n        self.initial_root_pos, self.initial_root_rot = self.root_pos.clone(), self.root_rot.clone()\n\n        # control parameters\n        self.thrusts = torch.zeros((self._num_envs, 4, 3), dtype=torch.float32, device=self._device)\n        self.thrust_cmds_damp = torch.zeros((self._num_envs, 4), dtype=torch.float32, device=self._device)\n        self.thrust_rot_damp = torch.zeros((self._num_envs, 4), dtype=torch.float32, device=self._device)\n\n        self.set_targets(self.all_indices)\n\n    def set_targets(self, env_ids):\n        num_sets = len(env_ids)\n        envs_long = env_ids.long()\n        # set target position randomly with x, y in (0, 0) and z in (2)\n        self.target_positions[envs_long, 0:2] = torch.zeros((num_sets, 2), device=self._device)\n        self.target_positions[envs_long, 2] = torch.ones(num_sets, device=self._device) * 2.0\n\n        # shift the target up so it visually aligns better\n        ball_pos = self.target_positions[envs_long] + self._env_pos[envs_long]\n        ball_pos[:, 2] += 0.0\n        self._balls.set_world_poses(ball_pos[:, 0:3], self.initial_ball_rot[envs_long].clone(), indices=env_ids)\n\n    def reset_idx(self, env_ids):\n        num_resets = len(env_ids)\n\n        self.dof_pos[env_ids, :] = torch_rand_float(-0.0, 0.0, (num_resets, self._copters.num_dof), device=self._device)\n        self.dof_vel[env_ids, :] = 0\n\n        root_pos = self.initial_root_pos.clone()\n        root_pos[env_ids, 0] += torch_rand_float(-0.0, 0.0, (num_resets, 1), device=self._device).view(-1)\n        root_pos[env_ids, 1] += torch_rand_float(-0.0, 0.0, (num_resets, 1), device=self._device).view(-1)\n        root_pos[env_ids, 2] += torch_rand_float(-0.0, 0.0, (num_resets, 1), device=self._device).view(-1)\n        root_velocities = self.root_velocities.clone()\n        root_velocities[env_ids] = 0\n\n        # apply resets\n        self._copters.set_joint_positions(self.dof_pos[env_ids], indices=env_ids)\n        self._copters.set_joint_velocities(self.dof_vel[env_ids], indices=env_ids)\n\n        self._copters.set_world_poses(root_pos[env_ids], self.initial_root_rot[env_ids].clone(), indices=env_ids)\n        self._copters.set_velocities(root_velocities[env_ids], indices=env_ids)\n\n        # bookkeeping\n        self.reset_buf[env_ids] = 0\n        self.progress_buf[env_ids] = 0\n\n        self.thrust_cmds_damp[env_ids] = 0\n        self.thrust_rot_damp[env_ids] = 0\n\n        # fill extras\n        self.extras[\"episode\"] = {}\n        for key in self.episode_sums.keys():\n            self.extras[\"episode\"][key] = torch.mean(self.episode_sums[key][env_ids]) / self._max_episode_length\n            self.episode_sums[key][env_ids] = 0.0\n\n    def calculate_metrics(self) -> None:\n        root_positions = self.root_pos - self._env_pos\n        root_quats = self.root_rot\n        root_angvels = self.root_velocities[:, 3:]\n\n        # pos reward\n        target_dist = torch.sqrt(torch.square(self.target_positions - root_positions).sum(-1))\n        pos_reward = 1.0 / (1.0 + target_dist)\n        self.target_dist = target_dist\n        self.root_positions = root_positions\n\n        # orient reward\n        ups = quat_axis(root_quats, 2)\n        self.orient_z = ups[..., 2]\n        up_reward = torch.clamp(ups[..., 2], min=0.0, max=1.0)\n\n        # effort reward\n        effort = torch.square(self.actions).sum(-1)\n        effort_reward = 0.05 * torch.exp(-0.5 * effort)\n\n        # spin reward\n        spin = torch.square(root_angvels).sum(-1)\n        spin_reward = 0.01 * torch.exp(-1.0 * spin)\n\n        # combined reward\n        self.rew_buf[:] = pos_reward + pos_reward * (up_reward + spin_reward) - effort_reward\n\n        # log episode reward sums\n        self.episode_sums[\"rew_pos\"] += pos_reward\n        self.episode_sums[\"rew_orient\"] += up_reward\n        self.episode_sums[\"rew_effort\"] += effort_reward\n        self.episode_sums[\"rew_spin\"] += spin_reward\n\n        # log raw info\n        self.episode_sums[\"raw_dist\"] += target_dist\n        self.episode_sums[\"raw_orient\"] += ups[..., 2]\n        self.episode_sums[\"raw_effort\"] += effort\n        self.episode_sums[\"raw_spin\"] += spin\n\n    def is_done(self) -> None:\n        # resets due to misbehavior\n        ones = torch.ones_like(self.reset_buf)\n        die = torch.zeros_like(self.reset_buf)\n        die = torch.where(self.target_dist > 5.0, ones, die)\n\n        # z >= 0.5 & z <= 5.0 & up > 0\n        die = torch.where(self.root_positions[..., 2] < 0.5, ones, die)\n        die = torch.where(self.root_positions[..., 2] > 5.0, ones, die)\n        die = torch.where(self.orient_z < 0.0, ones, die)\n\n        # resets due to episode length\n        self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die). Env Config is # used to create the object\nname: Crazyflie\n\nphysics_engine: ${..physics_engine}\n\n# if given, will override the device setting in gym.\nenv:\n  numEnvs: ${resolve_default:4096,${...num_envs}}\n  envSpacing: 2.5\n  maxEpisodeLength: 700\n  enableDebugVis: False\n\n  clipObservations: 5.0\n  clipActions: 1.0\n\nsim:\n  dt: 0.01\n  use_gpu_pipeline: ${eq:${...pipeline},\"gpu\"}\n  gravity: [0.0, 0.0, -9.81]\n  add_ground_plane: True\n  add_distant_light: False\n  use_fabric: True\n  enable_scene_query_support: False\n  # set to True if you use camera sensors in the environment\n  enable_cameras: False\n  disable_contact_processing: False\n\n  physx:\n    num_threads: ${....num_threads}\n    solver_type: ${....solver_type}\n    use_gpu: ${eq:${....sim_device},\"gpu\"} # set to False to run on CPU\n    solver_position_iteration_count: 6\n    solver_velocity_iteration_count: 0\n    contact_offset: 0.02\n    rest_offset: 0.001\n    bounce_threshold_velocity: 0.2\n    max_depenetration_velocity: 1000.0\n    friction_offset_threshold: 0.04\n    friction_correlation_distance: 0.025\n    enable_sleeping: True\n    enable_stabilization: False\n\n    # GPU buffers\n    gpu_max_rigid_contact_count: 524288\n    gpu_max_rigid_patch_count: 81920\n    gpu_found_lost_pairs_capacity: 4194304\n    gpu_found_lost_aggregate_pairs_capacity: 33554432\n    gpu_total_aggregate_pairs_capacity: 4194304\n    gpu_max_soft_body_contacts: 1048576\n    gpu_max_particle_contacts: 1048576\n    gpu_heap_capacity: 67108864\n    gpu_temp_buffer_capacity: 16777216\n    gpu_max_num_partitions: 8\n\n  crazyflie:\n    # -1 to use default values\n    override_usd_defaults: False\n    enable_self_collisions: True\n    enable_gyroscopic_forces: True\n    # also in stage params\n    # per-actor\n    solver_position_iteration_count: 6\n    solver_velocity_iteration_count: 0\n    sleep_threshold: 0.005\n    stabilization_threshold: 0.001\n    # per-body\n    density: -1\n    max_depenetration_velocity: 1000.0\n\n  ball:\n    # -1 to use default values\n    override_usd_defaults: False\n    make_kinematic: True\n    enable_self_collisions: False\n    enable_gyroscopic_forces: True\n    # also in stage params\n    # per-actor\n    solver_position_iteration_count: 6\n    solver_velocity_iteration_count: 0\n    sleep_threshold: 0.005\n    stabilization_threshold: 0.001\n    # per-body\n    density: -1\n    max_depenetration_velocity: 1000.0\nYour fellow now give you the following task: 1. **Explanation of the Task:**\n   - Our goal is to teach the drone to hover at a specific designated point in three-dimensional space. Key points are:\n     - **Hovering**: Maintain the drone at a precise position without any movement relative to external reference points.\n     - **Position**: The specific point (2,2,2) in a Cartesian coordinate system.\n     - **Speed Control**: The drone must move to this point at a fixed speed, in this case, 2m/s.\n\n2. **Nail Down the Parameters:**\n   - We will start by defining the following parameters:\n     - `initial_position`: The drone's starting coordinates.\n     - `target_position`: The coordinates (2,2,2) where the drone is to hover.\n     - `speed`: The constant speed the drone must maintain at 2m/s.\n     - `trajectory_plan`: The path the drone will take to reach `target_position` at the specified `speed`, assuming a straight-line trajectory.\n   - Detailed Task Description:\n     - Start from `initial_position`, take a straight path to `target_position`, at the end of which the drone will maintain a stable hover.\n\n3. **Teach Your Fellow to Write Good Code:**\n   - Implementing the Chain-of-Thought (CoT):\n     ```python\n     # Define drone's physical constants and initial conditions\n     initial_position = x0, y0, z0  # Tuple (or array) indicating the drone's starting position\n     target_position = (2, 2, 2)  # Target hover location as a tuple (or array)\n\n     # Speed settings\n     speed = 2  # m/s\n\n     # Function to move the drone to the target position\n     def fly_to_position(initial_pos, target_pos, speed):\n         # Move along a straight trajectory\n         trajectory = create_trajectory(initial_pos, target_pos, speed) \n         print(\"Move drone in a straight line towards the target position at speed:\", speed)\n\n     # Define a function for the drone to hover\n     def hover_at_position(target_pos):\n         # Code for hovering logic and safety checks\n         print(\"Drone is hovering at the target position:\", target_pos)\n\n     # Main operation flow\n     def operate_drone():\n         fly_to_position(initial_position, target_position, speed)\n         hover_at_position(target_position)\n     \n     # Execute operation\n     operate_drone()\n     ```\n   - Use comments to explain the idea or function of the code blocks.\n   - Variables should have explicitly inferred and clearly defined types and purposes.\n   - Follow an API guidebook or software development documentation.\n\n4. **Safety Considerations:**\n   - Validate external conditions such as no obstacles along the planned trajectory.\n   - Implement redundancy systems to maintain control if one fails.\n   - Ensure there is a clear and sufficient space for the drone to hover without any potential interference from human or other physical objects.\n   - Embed an emergency stop command that can be triggered if conditions deviate beyond safety tolerances.\n\n5. **Payment Strategy:**\n   - Base payment for the completed code implementation incorporating the Chain-of-Thought with comments and following safety guidelines.\n   - Bonus for innovative ideas in trajectory planning or hover control.\n   - Additional payment for above-benchmark performance in simulation tests.\n   - Incentive for correct implementation of safety and redundancy features.\n   - A tiered payment structure based on achieving certain objectives throughout the development stages, from simulation to a successful real-world hover test. Write a reward function for it. Remember, your fellow decides your payment.\nYou may refer to Human code difference as a runnable prototype for the task, however, if you think using this technique will ruin the diversity and randomness of the training, you may find another way out, I will pay you for your extra thought.    # Written by human, the parts of modification different from the original one, \n    # including new buffer contents, new task goal, and new reward computation.\n    def target_position_rot(self, root_positions, steps):\n        radius = 0.3\n        theta = torch.tensor(-2 * math.pi / 350 * steps)\n        center_z = 2.0\n\n        x = root_positions[:, 0]\n        z = root_positions[:, 2]-center_z\n        \n        # Calculate the scale factor to ensure the point lies on the circle with radius 0.5\n        scale = radius / torch.sqrt(x**2 + z**2)\n        \n        # Calculate x1 and z1 to make points lie on the circle\n        x1 = x * scale\n        z1 = z * scale\n        \n        # Construct the new points [x1, 0, z1] for each root_position\n        new_points = torch.stack([x1, torch.zeros_like(x1), z1], dim=1)\n        \n        # Calculate the cosine and sine of the rotation angle\n        cos_theta = torch.cos(theta)\n        sin_theta = torch.sin(theta)\n        \n        # Apply the rotation matrix for clockwise rotation in the XZ plane\n        x_rotated = new_points[:, 0] * cos_theta + new_points[:, 2] * sin_theta\n        z_rotated = -new_points[:, 0] * sin_theta + new_points[:, 2] * cos_theta\n        \n        # Construct the rotated points [x_rotated, 0, z_rotated]\n        rotated_points = torch.stack([x_rotated, new_points[:, 1], z_rotated], dim=1)\n        \n        return new_points, rotated_points\n    \n    def calculate_metrics(self) -> None:\n        root_positions = self.root_pos - self._env_pos\n        #root_positions[:, 0] -= 1\n        root_quats = self.root_rot\n        root_vel = self.root_velocities[:, :3]\n        root_angvels = self.root_velocities[:, 3:]\n        self.root_positions = root_positions\n        #self.root_positions[:, 0] += 1\n\n        rot_target = torch.zeros((self._num_envs, 3), device=self._device, dtype=torch.float32)\n        rot_target[:, 1] = 1\n\n        scaled_points, global_target_positions = self.target_position_rot(root_positions,0)\n        rotated_points_list = []\n        for i in range(4):\n            _, rotated_points = self.target_position_rot(root_positions, i)\n            rotated_points_list.append(rotated_points)\n\n        # Average the rotated points across the 4 steps\n        next_step = sum(rotated_points_list) / 4\n\n        target_dist = torch.norm(root_positions - global_target_positions, dim=1)\n        self.target_dist=target_dist\n\n        pos_reward = torch.exp(-3*self.target_dist)\n\n        # orient reward\n        norm_rot_target = rot_target / torch.norm(rot_target, dim=1, keepdim=True)\n\n        ups = quat_axis(root_quats, 2)\n        norm_ups = ups / torch.norm(ups, dim=1, keepdim=True)\n\n        cos_angle = torch.sum(norm_ups * norm_rot_target, dim=1)\n        coeff_rot=1-cos_angle\n        up_reward = torch.exp(-3*coeff_rot)\n\n        radius=0.3\n        desired_speed = 2*radius*math.pi / 3.5\n        current_speeds = torch.norm(root_vel, dim=1)  # Calculate the magnitudes of current velocities\n        speed_diff = torch.abs(current_speeds - desired_speed)  # Calculate the absolute difference from the desired speed\n        speed_reward = torch.exp(-3 * speed_diff)\n\n        traj = next_step - scaled_points\n\n        # Normalize root_vel and traj to unit vectors\n        norm_root_vel = root_vel / torch.norm(root_vel, dim=1, keepdim=True)\n        norm_traj = traj / torch.norm(traj, dim=1, keepdim=True)\n        # Calculate the dot product\n        dot_product = torch.sum(norm_root_vel * norm_traj, dim=1)\n        colinearity = 1 - dot_product\n\n        # Define the exponential reward\n        coline_reward = torch.exp(-3 * colinearity)\n\n        roll_angvel = root_angvels[:, 0]  # Extract roll component\n        yaw_angvel = root_angvels[:, 2]  # Extract yaw component\n\n        # Desired angular velocities for roll and yaw are zero (minimal movement)\n        desired_roll_angvel = 0.0\n        desired_yaw_angvel = 0.0\n\n        # Calculate the penalties for deviation from zero for roll and yaw\n        roll_penalty = torch.square(roll_angvel - desired_roll_angvel).sum(-1)\n        yaw_penalty = torch.square(yaw_angvel - desired_yaw_angvel).sum(-1)\n\n        y_angvel = root_angvels[:, 1]  # Y-axis angular velocity for spinning\n        desired_y_angvel = 2 * math.pi / 3.5  # Desired spin rate around Y-axis\n        y_angvel_diff = y_angvel - desired_y_angvel\n        y_penalty = torch.square(y_angvel_diff).sum(-1)\n\n        # Higher weight for Y-axis spin reward, lower weight for roll and yaw penalties to encourage minimal movements\n        combined_penalty = 0.1 * torch.exp(-1.0 * y_penalty) + 0.05 * torch.exp(-1.0 * roll_penalty) + 0.05 * torch.exp(-1.0 * yaw_penalty)\n\n        # Update spin_reward with the combined penalty\n        spin_reward = combined_penalty\n\n        # combined reward\n        self.rew_buf[:] = (pos_reward +speed_reward+coline_reward)(1+up_reward+spin_reward) \n        \n        # log episode reward sums\n        self.episode_sums[\"rew_pos\"] += pos_reward\n        self.episode_sums[\"rew_orient\"] += up_reward\n        self.episode_sums[\"rew_speed\"] += speed_reward\n        self.episode_sums[\"rew_coline\"] += coline_reward\n        self.episode_sums[\"rew_spin\"] += spin_reward\n\n        # log raw info\n        self.episode_sums[\"raw_dist\"] += target_dist\n        self.episode_sums[\"raw_orient\"] += ups[..., 2]\n\n    def is_done(self) -> None:\n        # resets due to misbehavior\n        ones = torch.ones_like(self.reset_buf)\n        die = torch.zeros_like(self.reset_buf)\n        die = torch.where(self.target_dist > 10.0, ones, die)\n\n        # z >= 0.5 & z <= 5.0 & up > 0\n        die = torch.where(self.root_positions[..., 2] < -2.0, ones, die)\n        die = torch.where(self.root_positions[..., 2] > 6.0, ones, die)\n\n        # resets due to episode length\n        self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die) \nYou have additional resources, which you're encouraged to follow and learn:\nUse API-existed functions first: def gf_quat_to_tensor(orientation: typing.Union[Gf.Quatd, Gf.Quatf, Gf.Quaternion], device=None) -> torch.Tensor:\n    # Converts a pxr Quaternion type to a torch array (scalar first).\n\ndef euler_angles_to_quats(euler_angles: torch.Tensor, degrees: bool = False, extrinsic: bool = True, device=None) -> torch.Tensor:\n    # Vectorized version of converting euler angles to quaternion (scalar first)\n\ndef rot_matrices_to_quats(rotation_matrices: torch.Tensor, device=None) -> torch.Tensor:\n    # Vectorized version of converting rotation matrices to quaternions\n\ndef rad2deg(radian_value: torch.Tensor, device=None) -> torch.Tensor:\n    # Converts radians to degrees and returns a tensor.\n\ndef deg2rad(degree_value: float, device=None) -> torch.Tensor:\n    # Converts degrees to radians and returns a tensor.\nUseful code outside Omniverse Isaac Sim: From: https://eureka-research.github.io/\nQuadcopter, best Eureka reward:\nimport torch\nfrom torch import Tensor\nfrom typing import Dict, Tuple\n\n@torch.jit.script\ndef compute_reward(root_positions: Tensor, root_linvels: Tensor, root_angvels: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n    quadcopter_x = root_positions[..., 0]\n    quadcopter_y = root_positions[..., 1]\n    quadcopter_z = root_positions[..., 2]\n\n    target_x = 0.0\n    target_y = 0.0\n    target_z = 1.0\n\n    squared_distance = (quadcopter_x - target_x)**2 + (quadcopter_y - target_y)**2 + (quadcopter_z - target_z)**2\n\n    linvel_magnitude = torch.sqrt(torch.sum(root_linvels**2, dim=-1))\n    angvel_magnitude = torch.sqrt(torch.sum(root_angvels**2, dim=-1))\n\n    # Change temperature parameters and re-scale\n    sq_distance_temperature = 1.0\n    linvel_temperature = 0.075  # Adjusted from 0.05 to 0.075\n    angvel_temperature = 0.005  # Adjusted from 0.01 to 0.005\n    angvel_penalty_temperature = 0.05  # Adjusted from 0.1 to 0.05\n\n    # Compute the reward components and apply transformations\n    sq_distance_reward = torch.exp(-sq_distance_temperature * squared_distance)\n    linvel_reward = torch.exp(-linvel_temperature * (linvel_magnitude - 0.5)) - 1.0  # Modified to encourage variance in linvel_reward\n    angvel_reward = torch.exp(-angvel_temperature * angvel_magnitude)\n    angvel_penalty = -torch.exp(angvel_penalty_temperature * (angvel_magnitude - 0.5))  # Modified to encourage variance in angvel_penalty\n\n    # Check success condition\n    success_condition = (squared_distance < 0.01).type(torch.float32)\n    success_reward = 15 * success_condition  # Increased the success reward from 10 to 15\n\n    # Calculate the total reward\n    total_reward = success_reward + sq_distance_reward + linvel_reward + angvel_reward + angvel_penalty\n\n    # Return the total reward and the components\n    return total_reward, {\"success_reward\": success_reward, \"sq_distance_reward\": sq_distance_reward, \"linvel_reward\": linvel_reward, \"angvel_reward\": angvel_reward, \"angvel_penalty\": angvel_penalty}\n\nFollow the output tips: - Since you are not allowed to modify other parts of the code, you can recreate the varibles and constants in the beginning of the calculate_metrics function, e.g., your target_position is no longer (0,0,2) initialized in the `set_target` but (4,4,4) to be created in `calculate_metrics`. Please make sure the tensor shape is match with the original one\n- You must make sure the self.xxx varible is defined, e.g., self.target_position or self.orient_z need be created in the calculate_metrics function. Your newly defined functions need to include self because they're the class method, plus, when calling them elsewhere, a self.your_new_function is necessary for Python\n- Since the whole pragram's operation is based on Tensor, please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n- Make sure any new tensor or variable you introduce is on the same device as the input tensors. \n- Ensure that tensor shape matches during calculation\n- If you create any new functions, then you must be sure to write the full function explicitly and return me the full code without any omission \n- You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components.\n- If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable. This is for tuning purpose\n- Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n- Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n\nYour output format. (just for the skeleton)\n\n```Python\n# New Code Starts Here\nimport math\nimport xxx\n\ndef name_your_new_function_here(xxx) -> None:\n    pass\n\ndef calculate_metrics(self) -> None:\n    self.new_function_here(xxx, yyy, zzz) # Remember to add self.name_your_new_function_here. It's in a class\n    ...\n    root_positions = self.root_pos - self._env_pos\n    ...\n    target_dist = torch.sqrt(torch.square(self.target_positions - root_positions).sum(-1))\n    ...\n    self.root_positions = root_positions\n    self.orient_z = ...\n    ...\n    spin = torch.square(root_angvels).sum(-1)\n    spin_reward = 0.01 * torch.exp(-0.1 * spin)\n    total_reward = pos_reward + pos_reward * (orient_reward + spin_reward) - effort_reward\n    self.rew_buf[:] = total_reward\n    self.episode_sums[\"rew_pos\"] += pos_reward\n    self.episode_sums[\"rew_orient\"] += orient_reward\n    self.episode_sums[\"rew_effort\"] += effort_reward\n    self.episode_sums[\"rew_spin\"] += spin_reward\n    self.episode_sums[\"raw_dist\"] += target_dist\n    self.episode_sums[\"raw_orient\"] += ups[..., 2]\n    self.episode_sums[\"raw_effort\"] += effort\n    self.episode_sums[\"raw_spin\"] += spin\n\n#END\n\n```\nAnd Must-do : ### What you must do:\n\nReturn me the code of functions with few or no comments, you need to make sure you have completed all functions according to the requirement without any omission, meaning that you should output runnable codes and not just comments waiting for completion by human, such that the code can run without error.\n\nSee under what circumstance you will be heavily punished in {If you output anything like the following, I will punish you and will not pay for that} sector.\n### If you output anything like the following, I will punish you and will not pay for that:\n\n```python\n    # Problems: 1. You don't need to return this function at all; 2. Funtions are poorly written and contains not runable trash\n    def set_up_scene(self, scene):\n        # Example: Add Crazyflie and target to the scene\n        self.get_crazyflie()\n        self.get_target()\n        # Add more scene setup steps here\n\n    def initialize_views(self, scene):\n        # Example: Initialize views for the Crazyflie and target for rendering or observation extraction\n        # This might be specific to how your simulation framework handles viewports and sensors\n\n    def get_crazyflie(self):\n        # Example: Instantiate Crazyflie drone within the environment\n        # Specific instantiation details depend on the simulation framework\n\n    def get_target(self):\n        # Example: Place a target object in the environment for the Crazyflie to interact with or move towards\n\n    def get_observations(self):\n        # Example: Collect and return current environmental observations relevant to the task\n        # Could include drone state, target position, obstacles, etc.\n\n    def pre_physics_step(self, actions):\n        # Process actions received from the neural network and apply to the drone\n        # This might involve converting action values to thrusts, rotations, etc.\n\n    def post_reset(self):\n        # Reset task-specific elements and prepare for a new episode\n        # This could involve repositioning the drone, resetting the target, etc.\n\n```\nAlso, the following cause error here: Reintroduce class\nensure no \"class\" or \"__init__\" is included in your output\n```python\n # New Code Starts Here\n    \n    import math\n    import numpy as np\n    import torch\n    from omni.isaac.core.utils.torch.rotations import *\n    EPS = 1e-6  # small constant to avoid divisions by 0 and log(0)\n    \n    \n    class CrazyflieTask(RLTask): \n        # Function that you newly created for calculation\n```\nYou should check whether your variables in is_done function is linked with self.variable, and you assign variable to the self.variable in the calculate_metrics part \nIf not: e.g. AttributeError: 'CrazyflieTask' object has no attribute 'target_dist'. It means you forget to link a variable with self.variable. Every time in your is_done function you use a variable, first is to ensure it is with self, second is that in previous functions you have already store it with self.\nAlso, I encourage you to not to make much comments after the code, we need to save tokens\nFinally: Add a sign for end of your code, when you finish the is_done part: #END\n"
    },
    {
        "role": "assistant",
        "content": "```python\n# New Code Starts Here\nimport torch\nimport math\n\ndef target_position_rot(self, root_positions):\n    target_pos = torch.tensor([2.0, 2.0, 2.0], dtype=torch.float32, device=self._device)\n    radius = 0.3  # The radius of the circle to hover around the target position\n    target_periphery_positions = torch.stack([\n        target_pos[0] + radius + root_positions[:, 0] - target_pos[0],\n        target_pos[1] + (2 * math.pi) * root_positions[:, 1] / 350,\n        target_pos[2] + radius + root_positions[:, 2] - target_pos[2],\n    ], dim=1)\n\n    # Normalize with the radius to keep the drone at the specified radius from the target position during hovering\n    scale_factors = radius / torch.norm(target_periphery_positions, dim=1, keepdim=True)\n    scaled_targets_periphery = target_periphery_positions * scale_factors\n\n    return target_pos, scaled_targets_periphery\n\ndef calculate_metrics(self) -> None:\n    self.root_positions = self.root_pos - self._env_pos\n    _, self.scaled_target_positions = self.target_position_rot(self.root_positions)\n\n    # Distance reward\n    target_dist = torch.norm(self.scaled_target_positions - self.root_positions, dim=1)\n    pos_reward = torch.exp(-1.5 * target_dist)\n\n    # orientation reward\n    ups = quat_axis(self.root_rot, 2)\n    self.orient_z = ups[:, 2]\n    up_reward = torch.exp(-2 * torch.square(-ups[:, 2] + 1))\n\n    # Speed penalty for fixed speed control\n    desired_speed = torch.norm(self.root_velocities[:, :3], dim=1) * 0.1\n    speed_diff = torch.abs(self.actions[:, 0] - desired_speed)\n    speed_reward = torch.exp(-0.1 * speed_diff)\n\n    # Tight hovering, stopped\n    hovering_reward = 0\n    if torch.allclose(self.root_velocities[:, :3], torch.zeros_like(self.root_velocities[:, :3]), atol=1e-5):\n        hovering_speed = 0.4  # Encourage maintaining a stationary velocity\n        hovering_reward = torch.exp(-hovering_speed)\n\n    # Effort reward (try to minimize control effort), reduce the control usage using the actions magnitude penalty\n    effort = torch.abs(torch.sum(self.actions, dim=1))\n    effort_reward = 0.05 * (1 / (1 + effort))  # Decrease reward smoothly as effort increases\n\n    # Combine rewards\n    self.rew_buf[:] = pos_reward + up_reward + speed_reward + hovering_reward - effort_reward\n\n    # Update logs\n    for kw in [\"pos\", \"orient\", \"speed\", \"effort\", \"hover\"]:\n        self.episode_sums[\"rew_{}\".format(kw)] += getattr(self, \"reward_{}\".format(kw))\n\n    for kw in [\"dist\", \"orient\"]:\n        self.episode_sums[\"raw_{}\".format(kw)] += getattr(self, \"metric_{}\".format(kw))\n\ndef is_done(self) -> None:\n    ones = torch.ones_like(self.reset_buf)\n    die = torch.zeros_like(self.reset_buf)\n\n    # z position outside window [0.5, 5.0]\n    die = torch.where(self.root_positions[:, 2] < 0.5, ones, die)\n    die = torch.where(self.root_positions[:, 2] > 5.0, ones, die)\n\n    # Falling off the periphery or being too fast\n    die = torch.where(self.root_velocities[:, :3].norm(dim=1) > 1.5, ones, die)\n\n    # Resets due to episode length\n    self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die)\n#END\n```"
    },
    {
        "role": "user",
        "content": ""
    }
]