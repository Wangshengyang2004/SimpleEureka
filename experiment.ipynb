{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import openai\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "# Import custom utilities\n",
    "from utils.extract_task_code import file_to_string\n",
    "from utils.simple_eureka import process_response\n",
    "from utils.system import (\n",
    "    check_system_encoding,\n",
    "    clean_folder,\n",
    "    copy_folder_sub,\n",
    ")\n",
    "from utils.misc import block_until_training\n",
    "from utils.agent import Agent\n",
    "from utils.tensorboard_parser import tensorboard_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, dict):\n",
    "                value = Config(value)\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__dict__}\"\n",
    "\n",
    "# Function to load YAML configuration\n",
    "def load_config(yaml_path):\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        config_dict = yaml.safe_load(file)\n",
    "    return Config(config_dict)\n",
    "\n",
    "# Example usage\n",
    "yaml_path = 'config/config.yaml'  # Replace with the path to your YAML file\n",
    "cfg = load_config(yaml_path)\n",
    "\n",
    "# Accessing the configuration\n",
    "print(cfg.gym)\n",
    "print(cfg.output.overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "ISAAC_ROOT_DIR = \"H:/Omniverse/Library/isaac_sim-2023.1.1/OmniIsaacGymEnvs/omniisaacgymenvs\"\n",
    "\n",
    "PYTHON_PATH = \"H:/Omniverse/Library/isaac_sim-2023.1.1/python.bat\"\n",
    "TASK = \"Crazyflie\"\n",
    "TASK_PATH = \"H:\\\\Omniverse\\\\Library\\\\isaac_sim-2023.1.1\\\\OmniIsaacGymEnvs\\\\omniisaacgymenvs\\\\tasks\\\\crazyflie.py\"\n",
    "SCRIPTS_DIR = \"scripts/rlgames_train.py\"\n",
    "HEADLESS = True\n",
    "ENABLE_RECORDING = False\n",
    "MULTIGPU = True\n",
    "\n",
    "model = \"gpt-4-turbo-2024-04-09\"\n",
    "key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup platform and logger\n",
    "platform = sys.platform\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logger = logger.opt(colors=True)\n",
    "logger.add(\n",
    "    sink=f\"./results/{now}/output.log\",\n",
    "    enqueue=True,\n",
    "    backtrace=True,\n",
    "    diagnose=True,\n",
    ")\n",
    "\n",
    "EUREKA_ROOT_DIR = os.getcwd()\n",
    "RESULT_DIR = f\"{EUREKA_ROOT_DIR}/results/{now}\"\n",
    "\n",
    "# Clean the results directory\n",
    "shutil.rmtree(RESULT_DIR, ignore_errors=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "prompt_dir = f\"{EUREKA_ROOT_DIR}/prompts\"\n",
    "logger.debug(f\"Using LLM: {model} with API Key: {key}\")\n",
    "code_feedback = file_to_string(f\"{prompt_dir}/code_feedback.txt\")\n",
    "task_obs_code_string = file_to_string(f\"{EUREKA_ROOT_DIR}/input/{TASK}.py\")\n",
    "policy_feedback = file_to_string(f\"{prompt_dir}/policy_feedback.txt\")\n",
    "execution_error_feedback = file_to_string(\n",
    "    f\"{prompt_dir}/execution_error_feedback.txt\"\n",
    ")\n",
    "code_output_tip = file_to_string(f\"{prompt_dir}/actor/code_output_tip.txt\")\n",
    "DUMMY_FAILURE = -10000.0\n",
    "max_successes = []\n",
    "max_successes_reward_correlation = []\n",
    "execute_rates = []\n",
    "best_code_paths = []\n",
    "max_success_overall = DUMMY_FAILURE\n",
    "max_success_reward_correlation_overall = DUMMY_FAILURE\n",
    "max_reward_code_path = None\n",
    "clean_folder(f\"{ISAAC_ROOT_DIR}/runs/{TASK}\")\n",
    "\n",
    "# ---------------------- MESSAGE Assemble------------------#\n",
    "actor_prompt = Agent(cfg=cfg)\n",
    "messages = actor_prompt.message()\n",
    "full_prompt = messages[0][\"content\"] + messages[1][\"content\"]\n",
    "logger.info(\"Full Prompt: \" + full_prompt)\n",
    "\n",
    "# Save the full prompt to a file under {EUREKA_ROOT_DIR}/results\n",
    "with open(f\"{RESULT_DIR}/full_prompt.txt\", \"w\") as f:\n",
    "    f.write(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Evolution ------------------#\n",
    "iter = 0  # Example iteration number, you can loop through multiple iterations\n",
    "BASE_DIR = f\"{RESULT_DIR}/iter{iter}\"\n",
    "# Make sub directories: code, reponses, tensorboard, and videos\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{BASE_DIR}/{iter}\", exist_ok=True)\n",
    "\n",
    "responses = []\n",
    "response_cur = None\n",
    "total_samples = 0\n",
    "total_token = 0\n",
    "total_completion_token = 0\n",
    "chunk_size: int = cfg.generation.chunk_size\n",
    "logger.info(\n",
    "    f\"Iteration {iter}: Generating {cfg.generation.sample} samples with {cfg.api.model}\"\n",
    ")\n",
    "client = openai.OpenAI(api_key=cfg.api.key, base_url=cfg.api.url)\n",
    "while True:\n",
    "    if total_samples >= cfg.generation.sample:\n",
    "        break\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response_cur = client.chat.completions.create(\n",
    "                model=cfg.api.model,\n",
    "                messages=messages,\n",
    "                temperature=cfg.api.temperature,\n",
    "                max_tokens=cfg.api.max_tokens,\n",
    "                n=chunk_size,\n",
    "            )\n",
    "            total_samples += chunk_size\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt >= 10:\n",
    "                chunk_size = max(int(chunk_size / 2), 1)\n",
    "                print(\"Current Chunk Size\", chunk_size)\n",
    "            logger.info(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            time.sleep(1)\n",
    "    if response_cur is None:\n",
    "        logger.info(\"Code terminated due to too many failed attempts!\")\n",
    "        break\n",
    "\n",
    "    responses.extend(response_cur.choices)\n",
    "    prompt_tokens = response_cur.usage.prompt_tokens\n",
    "    total_completion_token += response_cur.usage.completion_tokens\n",
    "    total_token += response_cur.usage.total_tokens\n",
    "\n",
    "if cfg.generation.sample == 1:\n",
    "    logger.info(\n",
    "        f\"Iteration {iter}: GPT Output:\\n \"\n",
    "        + responses[0].message.content\n",
    "        + \"\\n\"\n",
    "    )\n",
    "\n",
    "# logger Token Information\n",
    "logger.info(\n",
    "    f\"Iteration {iter}: Prompt Tokens: {prompt_tokens}, Completion Tokens: {total_completion_token}, Total Tokens: {total_token}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for code and RL runs\n",
    "code_runs = []\n",
    "rl_runs = []\n",
    "\n",
    "for response_id in range(cfg.generation.sample):\n",
    "    # Clean up the omniverse isaac sim environment's output directory\n",
    "    clean_folder(f\"{ISAAC_ROOT_DIR}/runs/{TASK}\")\n",
    "    os.makedirs(f\"{BASE_DIR}/{response_id}\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE_DIR}/{response_id}/checkpoint\", exist_ok=True)\n",
    "    response_cur = responses[response_id].message.content\n",
    "    \n",
    "    # Save the response to a file\n",
    "    with open(\n",
    "        f\"{BASE_DIR}/{response_id}/response.txt\", \"w\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        f.write(response_cur)\n",
    "    logger.info(f\"Iteration {iter}: Processing Code Run {response_id}\")\n",
    "\n",
    "    # Regex patterns to extract python code enclosed in GPT response\n",
    "    patterns = [\n",
    "        r\"```python(.*?)```\",\n",
    "        r\"```(.*?)```\",\n",
    "        r'\"\"\"(.*?)\"\"\"',\n",
    "        r'\"\"(.*?)\"\"',\n",
    "        r'\"(.*?)\"',\n",
    "    ]\n",
    "    code_string = None\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_cur, re.DOTALL)\n",
    "        if match:\n",
    "            code_string = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Use the entire response if no code block is found\n",
    "    code_string = response_cur if code_string is None else code_string\n",
    "\n",
    "    # Remove unnecessary imports and add indentation\n",
    "    lines = code_string.split(\"\\n\")\n",
    "    lines = [\" \" * 4 + line for line in lines]\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):\n",
    "            code_string = \"\\n\".join(lines[i:])\n",
    "            break\n",
    "\n",
    "    code_runs.append(code_string)\n",
    "\n",
    "    # Process the response code to obtain the final code string\n",
    "    code_string, reward_only_string = process_response(\n",
    "        code_string, task_obs_code_string\n",
    "    )\n",
    "    \n",
    "    # Save the new environment code\n",
    "    output_file = f\"{BASE_DIR}/{response_id}/env_iter{iter}_response{response_id}.py\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.writelines(code_string + \"\\n\")\n",
    "\n",
    "    with open(\n",
    "        f\"{BASE_DIR}/{response_id}/env_iter{iter}_response{response_id}_rewardonly.py\",\n",
    "        \"w\",\n",
    "    ) as file:\n",
    "        file.writelines(reward_only_string + \"\\n\")\n",
    "\n",
    "    shutil.copyfile(output_file, f\"{TASK_PATH}\")\n",
    "    std_path = f\"{BASE_DIR}/{response_id}/env_iter{iter}_response{response_id}_train.log\"\n",
    "    with open(std_path, \"w\") as f:\n",
    "        encoding = \"utf-8\" if check_system_encoding() else \"gbk\"\n",
    "\n",
    "        if platform == \"win32\":\n",
    "            driver = cfg.gym.omniisaacsimpathenv.split(\":\")[0]\n",
    "            if MULTIGPU:\n",
    "                command = f\"{driver}: & cd {ISAAC_ROOT_DIR} & {PYTHON_PATH} {SCRIPTS_DIR} task={TASK} headless={HEADLESS} multigpu={MULTIGPU} \"\n",
    "            else:\n",
    "                command = f\"{driver}: & cd {ISAAC_ROOT_DIR} & {PYTHON_PATH} {SCRIPTS_DIR} task={TASK} headless={HEADLESS} \"\n",
    "        elif platform == \"linux\":\n",
    "            if MULTIGPU:\n",
    "                command = f\"cd {ISAAC_ROOT_DIR} && {PYTHON_PATH} {SCRIPTS_DIR} task={TASK} headless={HEADLESS} multigpu={MULTIGPU} \"\n",
    "            else:\n",
    "                command = f\"cd {ISAAC_ROOT_DIR} && {PYTHON_PATH} {SCRIPTS_DIR} task={TASK} headless={HEADLESS} \"\n",
    "        else:\n",
    "            logger.error(\"Unsupported platform!\")\n",
    "            exit()\n",
    "\n",
    "        if ENABLE_RECORDING and not MULTIGPU:\n",
    "            os.makedirs(f\"{BASE_DIR}/{response_id}/videos/\", exist_ok=True)\n",
    "            command += f\"enable_recording=True recording_dir={BASE_DIR}/{response_id}/videos/\"\n",
    "        else:\n",
    "            logger.info(\"Recording is disabled! Either enable recording or use multi-gpu training!\")\n",
    "        \n",
    "        logger.info(f\"Command: {command}\")\n",
    "        process = subprocess.Popen(\n",
    "            command, shell=True, stdout=f, stderr=f, encoding=encoding\n",
    "        )\n",
    "    block_until_training(\n",
    "        std_path,\n",
    "        success_keyword=cfg.env.success_keyword,\n",
    "        failure_keyword=cfg.env.failure_keyword,\n",
    "        log_status=True,\n",
    "        iter_num=iter,\n",
    "        response_id=response_id,\n",
    "    )\n",
    "    copy_folder_sub(\n",
    "        f\"{ISAAC_ROOT_DIR}/runs/{TASK}\", f\"{BASE_DIR}/{response_id}/\"\n",
    "    )\n",
    "    rl_runs.append(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicate with all RL runs to ensure they are completed\n",
    "for response_id, (code_run, rl_run) in enumerate(zip(code_runs, rl_runs)):\n",
    "    rl_run.communicate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
