### Tips
- No need to comment on the code, it will save some tokens
- Don't forget to set your newly created variable with self.x, or forget to set the self.variable = variable. Very import, otherwise I will deduct your payment
- Since the whole pragram's operation is based on Tensor, please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). 
- Make sure any new tensor or variable you introduce is on the same device as the input tensors. 
- Ensure that tensor shape matches during calculation, otherwise this will ruin the full code 
- Make sure the Tensor is on the same device by .to(self.device) method
- If you create any new functions, then you must be sure to write the full function explicitly and return me the full code. You are responsible for writing runnable code without any omission 
- You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components.
-  If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable. This is for tuning purpose
- Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor
- Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.

- For quaternion conversion, you should use the official functions:
```Python
def gf_quat_to_tensor(orientation: typing.Union[Gf.Quatd, Gf.Quatf, Gf.Quaternion], device=None) -> torch.Tensor:
    # Converts a pxr Quaternion type to a torch array (scalar first).

def euler_angles_to_quats(euler_angles: torch.Tensor, degrees: bool = False, extrinsic: bool = True, device=None) -> torch.Tensor:
    # Vectorized version of converting euler angles to quaternion (scalar first)

def rot_matrices_to_quats(rotation_matrices: torch.Tensor, device=None) -> torch.Tensor:
    # Vectorized version of converting rotation matrices to quaternions

def rad2deg(radian_value: torch.Tensor, device=None) -> torch.Tensor:
    # Converts radians to degrees and returns a tensor.

def deg2rad(degree_value: float, device=None) -> torch.Tensor:
    # Converts degrees to radians and returns a tensor.
```

Follow the structure below for output can bring you extra money, just the structure (function names and input/output) for you to refer, be creative of the detailed coding:
"
```python
# New Code Starts Here
import math
import xxx

self.episode_sums = {
        "rew_pos": torch_zeros(),
        "rew_orient": torch_zeros(),
        "rew_effort": torch_zeros(),
        "rew_spin": torch_zeros(),
        ... (more if you need)
        "raw_dist": torch_zeros(),
        "raw_orient": torch_zeros(),
        "raw_effort": torch_zeros(),
        "raw_spin": torch_zeros(),
    }

def name_your_new_function_here(self, xxx) -> None:
    pass

def calculate_metrics(self) -> None:
    self.new_function_here()
    ...
    root_positions = self.root_pos - self._env_pos
    root_quats = self.root_rot
    root_angvels = self.root_velocities[:, 3:]
    target_dist = torch.sqrt(torch.square(self.target_positions - root_positions).sum(-1))
    pos_reward = 1.0 / (1.0 + 0.1 * target_dist)
    ups = quat_axis(root_quats, 2)
    orient_reward = torch.clamp(ups[..., 2], min=0.0, max=1.0)
    effort = torch.square(self.actions).sum(-1)
    effort_reward = 0.05 * torch.exp(-0.1 * effort)
    ...
    spin = torch.square(root_angvels).sum(-1)
    spin_reward = 0.01 * torch.exp(-0.1 * spin)
    total_reward = pos_reward + pos_reward * (orient_reward + spin_reward) - effort_reward
    self.rew_buf[:] = total_reward
    self.episode_sums["rew_pos"] += pos_reward
    self.episode_sums["rew_orient"] += orient_reward
    self.episode_sums["rew_effort"] += effort_reward
    self.episode_sums["rew_spin"] += spin_reward
    self.episode_sums["raw_dist"] += target_dist
    self.episode_sums["raw_orient"] += ups[..., 2]
    self.episode_sums["raw_effort"] += effort
    self.episode_sums["raw_spin"] += spin

def is_done(self) -> None:
    ones = torch.ones_like(self.reset_buf)
    die = torch.zeros_like(self.reset_buf)
    die = torch.where(self.target_dist > 5.0, ones, die)
    die = torch.where(self.root_positions[..., 2] < 0.5, ones, die)
    die = torch.where(self.root_positions[..., 2] > 5.0, ones, die)
    die = torch.where(self.orient_z < 0.0, ones, die)
    self.reset_buf[:] = torch.where(self.progress_buf >= self._max_episode_length - 1, ones, die)

```
#END
"