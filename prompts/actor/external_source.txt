@torch.jit.script
def compute_success(root_positions, root_quats, root_linvels, root_angvels, reset_buf, consecutive_successes, progress_buf, max_episode_length):
    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, float) -> Tuple[Tensor, Tensor, Tensor]

    target_dist = torch.sqrt(root_positions[..., 0] * root_positions[..., 0] +
                             root_positions[..., 1] * root_positions[..., 1] +
                             (1 - root_positions[..., 2]) * (1 - root_positions[..., 2]))
    pos_reward = 1.0 / (1.0 + target_dist * target_dist)

    ups = quat_axis(root_quats, 2)
    tiltage = torch.abs(1 - ups[..., 2])
    up_reward = 1.0 / (1.0 + tiltage * tiltage)

    spinnage = torch.abs(root_angvels[..., 2])
    spinnage_reward = 1.0 / (1.0 + spinnage * spinnage)

    reward = pos_reward + pos_reward * (up_reward + spinnage_reward)

    ones = torch.ones_like(reset_buf)
    die = torch.zeros_like(reset_buf)
    die = torch.where(target_dist > 3.0, ones, die)
    die = torch.where(root_positions[..., 2] < 0.3, ones, die)

    consecutive_successes = -target_dist.mean()
    reset = torch.where(progress_buf >= max_episode_length - 1, ones, die)

    return reward, reset, consecutive_successes
    
@torch.jit.script
def compute_reward(root_positions: torch.Tensor, root_angvels: torch.Tensor, root_quats: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    # Adjust scale and temperature values based on feedback
    position_temp: float = 0.1  # Keep as is since it seems effective
    flip_temp: float = 0.2  # Increased to provide smoother adjustments

    # Positional error (the drone should stay near (0, 0, 1))
    target_position = torch.tensor([0.0, 0.0, 1.0], device=root_positions.device)
    position_error = torch.norm(target_position - root_positions, dim=-1)
    position_reward = torch.exp(-position_temp * position_error ** 2)

    # Refine backflip success metric using angular velocity around Y-axis
    desired_angvel_y = 10.0  # Threshold for effective backflip
    flip_velocity_error = torch.abs(root_angvels[..., 1] - desired_angvel_y)
    soft_flip_reward = torch.sigmoid(-flip_temp * flip_velocity_error + 5)  # Added offset to initialize learning

    # Combined reward aiming for stability and successful flip
    total_reward = position_reward + soft_flip_reward
    
    # Reward components for debugging
    reward_components = {
        "position_reward": position_reward,
        "soft_flip_reward": soft_flip_reward
    }
    
    return total_reward, reward_components